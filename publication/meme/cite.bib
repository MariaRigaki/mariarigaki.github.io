@InProceedings{10.1007/978-3-031-51482-1_3,
author="Rigaki, Maria
and Garcia, Sebastian",
editor="Tsudik, Gene
and Conti, Mauro
and Liang, Kaitai
and Smaragdakis, Georgios",
title="The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning",
booktitle="Computer Security -- ESORICS 2023",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="44--64",
abstract="Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection toolchain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32--73{\%}. It also produces surrogate models with a prediction label agreement with the respective target models between 97--99{\%}. The surrogate could be used to fine-tune and improve the evasion rate in the future.",
isbn="978-3-031-51482-1"
}
