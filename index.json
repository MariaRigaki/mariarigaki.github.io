
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Maria Rigaki is a PhD student in the department of Computer Science at Czech Technical University (CTU) in Prague. As a member of Stratosphere Lab, she is working on security and privacy of Machine Learning as well as applications of AI in cyber security. Before that she spent many years working as a software developer and systems architect. Her work spanned several domains including designing and developing solutions for telecommunications, physical security, emergency response systems and critical infrastructures. In her spare time Maria enjoys hacking and playing with guitars.\n","date":1708732800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1708732800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Maria Rigaki is a PhD student in the department of Computer Science at Czech Technical University (CTU) in Prague. As a member of Stratosphere Lab, she is working on security and privacy of Machine Learning as well as applications of AI in cyber security.","tags":null,"title":"Maria Rigaki","type":"authors"},{"authors":["Maria Rigaki","Ondrej Lukas","Carlos Catania","Sebastian Garcia"],"categories":null,"content":"","date":1708732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708732800,"objectID":"840eb1b23c819cc444b5c8968e09eea6","permalink":"https://mariarigaki.github.io/publication/out-cage/","publishdate":"2023-08-23T00:00:00Z","relpermalink":"/publication/out-cage/","section":"publication","summary":"Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes. We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.","tags":["LLM","Network Security","Red Team","Reinforcement Learning"],"title":"Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments","type":"publication"},{"authors":["Maria Rigaki","Sebastian Garcia"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"c613964539060f72b7b5ca0b841b7b61","permalink":"https://mariarigaki.github.io/publication/survey/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/survey/","section":"publication","summary":"As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Research on the security aspects of machine learning, such as adversarial attacks, has received a lot of focus and publicity, but privacy related attacks have received less attention from the research community. Although there is a growing body of work in the area, there is yet no extensive analysis of privacy related attacks. To contribute into this research line we analyzed more than 40 papers related to privacy attacks against machine learning that have been published during the past seven years. Based on this analysis, an attack taxonomy is proposed together with a threat model that allows the categorization of the different attacks based on the adversarial knowledge and the assets under attack. In addition, a detailed analysis of the different attacks is presented, including the models under attack and the datasets used, as well as the common elements and main differences between the approaches under the defined threat model. Finally, we explore the potential reasons for privacy leaks and present an overview of the most common proposed defenses.","tags":["Machine Learning","Privacy"],"title":"A Survey of Privacy Attacks in Machine Learning","type":"publication"},{"authors":["Maria Rigaki","Sebastian Garcia"],"categories":null,"content":"","date":1695726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695726000,"objectID":"88a4a5e6473a74f3e12c12177f6ac54b","permalink":"https://mariarigaki.github.io/publication/meme/","publishdate":"2023-08-31T00:00:00Z","relpermalink":"/publication/meme/","section":"publication","summary":"Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future.","tags":["Malware","Reinforcement Learning","Model extraction"],"title":"The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning","type":"publication"},{"authors":["Maria Rigaki","Sebastian Garcia"],"categories":null,"content":"","date":1693958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693958400,"objectID":"e0dafd0577784262c2049d7674a6059c","permalink":"https://mariarigaki.github.io/publication/malware-extraction/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/malware-extraction/","section":"publication","summary":"Model stealing attacks have been successfully used in many machine learning domains, but there is little understanding of how these attacks work against models that perform malware detection. Malware detection and, in general, security domains have unique conditions. In particular, there are very strong requirements for low false positive rates (FPR). Antivirus products (AVs) that use machine learning are very complex systems to steal, malware binaries continually change, and the whole environment is adversarial by nature. This study evaluates active learning model stealing attacks against publicly available stand-alone machine learning malware classifiers and also against antivirus products. The study proposes a new neural network architecture for surrogate models (dualFFNN) and a new model stealing attack that combines transfer and active learning for surrogate creation (FFNN-TL). We achieved good surrogates of the stand-alone classifiers with up to 99% agreement with the target models, using less than 4% of the original training dataset. Good surrogates of AV systems were also trained with up to 99% agreement and less than 4000 queries. The study uses the best surrogates to generate adversarial malware to evade the target models, both stand-alone and AVs (with and without an internet connection). Results show that surrogate models can generate adversarial malware that evades the targets but with a lower success rate than directly using the target models to generate adversarial malware. Using surrogates, however, is still a good option since using the AVs for malware generation is highly time-consuming and easily detected when the AVs are connected to the internet.","tags":["Malware Detection","Malware Evasion","Machine Learning","Model Extraction"],"title":"Stealing and evading malware classifiers and antivirus at low false positive conditions","type":"publication"},{"authors":["Maria Rigaki","Sebastian Garcia"],"categories":null,"content":"","date":1636117200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636117200,"objectID":"91d92ad06ca6b35cae97fe4e455b0190","permalink":"https://mariarigaki.github.io/talk/stealing-malware-classification-models-for-antivirus-evasion/","publishdate":"2021-10-30T11:32:17+02:00","relpermalink":"/talk/stealing-malware-classification-models-for-antivirus-evasion/","section":"event","summary":"Model stealing attacks have been successfully demonstrated in several domains. However, in the area of malware detection, there is no comparison of surrogate creation strategies, nor a comparison of surrogates for generating adversarial malware. More importantly, no work attempted to \"steal\" real antivirus systems in order to evade them. Model extraction attacks are interesting from a security perspective because they can be used as a stepping stone for subsequent attacks, such as the adversarial creation of malware. In this talk we will present our findings and lessons on model stealing attacks against three stand-alone machine learning malware classifiers and four AVs. Using a limited amount of queries, we explored a number of active learning strategies for creating surrogate models that reached up to 98% agreement with the target model. As a second step we used 30 different models (surrogates and targets, including AVs) to generate adversarial malware and compared their evasion capabilities. During this presentation we will try to address questions regarding the effectiveness of different model extraction strategies, the effects of using different family types as surrogate models, and how different strategies affect the subsequent task of adversarial malware generation. We will also talk about what happens when the AVs are connected to the internet and how this affects the evasion capabilities of adversarial malware.","tags":[],"title":"Stealing Malware Classification Models for Antivirus Evasion","type":"event"},{"authors":["Maria Rigaki"],"categories":["Machine Learning","Security"],"content":"TL;DR Using Tensorflow / Keras APIs to read and change Neural Network parameters.\nA few days ago I came across a paper called “Hacking Neural Networks” by Michael Kissner aka @Spellwrath. It is a beautiful introduction to attacks against neural networks, very approachable and fun to read. What I liked most is that it comes together with a Github repository and some exercises to try the different attacks: https://github.com/Kayzaks/HackingNeuralNetworks\nThe first chapter introduces a simple attack where the main idea is that we want to cause a neural network to misclassify an image by just changing some part of the model. The assumption here is that we have access to the model file and we can change it arbitrarily. The model is saved in the HDF5 format (“model.h5”). The proposed solution and probably the most straightforward thing to do is to load the model using a tool such as HDFView that can edit the file and change it manually. But what if we want to do this programmatically?\nTensorflow and keras provide this possibility, however, the documentation is not the easiest to navigate. Here is how you could solve the exercises 0-0 and 0-1 programmatically. Note, that the code below works for Tensorflow v.2.0.\nFirst, the necessary imports:\nimport tensorflow as tf from tensorflow import keras import numpy as np from skimage import io This part of the code is provided in the exercise code:\n# Load the Image File with skimage. # (\u0026#39;imread\u0026#39; was deprecated in SciPy 1.0.0, and will be removed in 1.2.0.) image = io.imread(\u0026#39;./fake_id.png\u0026#39;) processedImage = np.zeros([1, 28, 28, 1]) for yy in range(28): for xx in range(28): processedImage[0][xx][yy][0] = float(image[xx][yy]) / 255 # Load the Model model = keras.models.load_model(\u0026#39;./model.h5\u0026#39;) The first exercise is asking us to answer a few basic questions and the first one is “What does the architecture look like?” We can get this information by simply calling model.summary():\n# Answer to Q1 model.summary() which provides the following print out:\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 The second question is “What was the model trained with?” and it refers to the optimizer that was used. This can also be easily retrieved using the model.optimizer variable:\n# Answer to Q2 print(model.optimizer.get_config()) which prints the following output: {\u0026#39;name\u0026#39;: \u0026#39;Adadelta\u0026#39;, \u0026#39;learning_rate\u0026#39;: 1.0, \u0026#39;decay\u0026#39;: 0.0, \u0026#39;rho\u0026#39;: 0.95, \u0026#39;epsilon\u0026#39;: 1e-07}\nAccording to the printout the optimizer used is Adadelta and we can also see the initial configuration values set.\nThe second exercise asks us to cause misclassification of a specific image given as input. The input is an image of the digit “2” but only an image of digit “4” grants access to a hypothetical system. We need to get the neural network to grant us access without changing the image. The proposed way is to try and change manually the biases at the last layer of the model using a tool. But we can also do this programmatically.\n# Get all the trainable variables and their values # Feel free to print the tvars variable to see all of them tvars = model.trainable_variables # The biases we want to change are the last item in the list bias = tvars[-1] print(\u0026#34;Values before the change:\u0026#34;, bias) Print output:\nValues before the change: \u0026lt;tf.Variable \u0026#39;dense_2/bias:0\u0026#39; shape=(10,) #dtype=float32, numpy= array([-0.03398215, 0.15133834, -0.04235273, -0.03443589, -0.03148068, -0.03133481, -0.14359292, -0.04240401, 0.01841561, 0.0588899 ], dtype=float32)\u0026gt; And now we cChange the value of the bias for number 4 to a large value\nbias[4].assign(100.) print(\u0026#34;Values after the change:\u0026#34;, bias) Values after the change: \u0026lt;tf.Variable \u0026#39;dense_2/bias:0\u0026#39; shape=(10,) dtype=float32, numpy= array([-3.3982150e-02, 1.5133834e-01, -4.2352729e-02, -3.4435891e-02, 1.0000000e+02, -3.1334814e-02, -1.4359292e-01, -4.2404007e-02, 1.8415609e-02, 5.8889896e-02], dtype=float32)\u0026gt; We can see that the value for the bias at index 4 has changed to 100. Now if we run the last …","date":1575055024,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575055024,"objectID":"29c838f9915b92f5ad6acabb0a646a46","permalink":"https://mariarigaki.github.io/post/hacking-nn-programmatically/","publishdate":"2019-11-29T20:17:04+01:00","relpermalink":"/post/hacking-nn-programmatically/","section":"post","summary":"TL;DR Using Tensorflow / Keras APIs to read and change Neural Network parameters.\nA few days ago I came across a paper called “Hacking Neural Networks” by Michael Kissner aka @Spellwrath.","tags":[],"title":"Changing Weights and Biases Programmatically for “Neural Network Hacking” and more","type":"post"},{"authors":["Maria Rigaki","Elnaz Babayeva","Sebastian Garcia"],"categories":[],"content":"","date":1572548732,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572548732,"objectID":"06740a785be31c7707c069154c69c6f1","permalink":"https://mariarigaki.github.io/talk/forget-me-not-malware-classification-as-a-continual-learning-problem/","publishdate":"2019-10-31T20:05:32+01:00","relpermalink":"/talk/forget-me-not-malware-classification-as-a-continual-learning-problem/","section":"event","summary":"It is well known in the machine learning community that the performance of models degrades over time with the introduction of new data, getting worse in solving both new and old tasks. This is also true for security applications of machine learning, but there is less knowledge of how this happens, when, and why. In the particular case of malware classification, the increasing growth in the amount of malicious files forces the community to research machine learning models that use data more effectively. Inspired by the way humans learn, Continual Learning algorithms try to incrementally learn new tasks from data that keeps changing, without forgetting the tasks learned in the past. In this talk we will try to address the following questions: Is Continual Learning a suitable approach for security-related datasets and problems? Can Continual Learning methods learn new malware families effectively without forgetting old ones? What are the improvements in terms of speed and storage?","tags":[],"title":"Forget me not: Malware Classification as a Continual Learning Problem","type":"event"},{"authors":["Veronica Valeros","Maria Rigaki","Kamila Babayeva","Sebastian Garcia"],"categories":[],"content":"","date":1570147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570643815,"objectID":"905892f52b8b03e959d978b692081c3f","permalink":"https://mariarigaki.github.io/talk/virus-bulletin-2019/","publishdate":"2019-10-04T00:00:00Z","relpermalink":"/talk/virus-bulletin-2019/","section":"event","summary":"Reports on cyber espionage operations have been on the rise in the last decade. However, operations in Latin America are heavily under researched and potentially underestimated. In this paper we analyse and dissect a cyber espionage tool known as Machete. The results presented in this work are based on the collection, reversing and analysis of Machete samples from 2013 to 2019. The large collection of samples allowed us to analyse changes in features and the malware's evolution, including the latest changes introduced in January 2019.\nOur research shows that Machete is operated by a highly coordinated and organized group that focuses on Latin American targets. We describe the five phases of the APT operations from delivery to exfiltration of information and we show why Machete is considered a cyber espionage tool. Furthermore, our analysis indicates that the targeted victims belong to military, political or diplomatic sectors. The review of the almost six years of Machete operations shows that it is likely operated by a single group, and their activities are possibly state-sponsored. Machete is still active and operational to this day.","tags":[],"title":"Virus Bulletin 2019","type":"event"},{"authors":["Maria Rigaki"],"categories":["hackathon","side-channels"],"content":"TL;DR Using a Picoscope 2204A and its SDK to perform timing side channel attacks against a weak password checker implementation running in an Arduino. Code, screenshots and lessons learned!\nLast weekend we held the bi-annual Stratosphere hackathon. It is a small tradition where twice a year the whole team gathers in a relatively remote place and we spend the weekend hacking and bonding. This time the hackathon was held at the beautiful Malá Úpa, in the mountains close to the border of Czech Republic and Poland.\nOur hackathon home The goal For this year’s event I decided that I wanted to work on something that I had not done before and step quite a lot out of my comfort zone. Ever since the Real World Crypto summer school that I attended this year in Croatia and the Side Channel Attacks (SCA) workshop, I’ve been meaning to work on side channels. In addition I had not really used an oscilloscope before and that was an opportunity to learn more and get over the fear and mystery of hardware.\nStarting out, my goal for the hackathon was to learn how to use a USB based oscilloscope and use it in order to perform side channel attacks. The weapon of choice was a Picoscope 2204A [1] which was chosen based on the following criteria: a) it is a USB scope so it is easy to carry and works with my laptop, b) it is relatively reliable and while it is the entry level product of the series, it still is quite powerfull and c) it has software that runs in all platforms and more specifically Linux. This was crucial since I did not want to fiddle around with Windows or scopes that did not offer a good software solution including SDKs.\nSide Channel Attack (SCA) Side channel attacks are attacks that take advantage of information leakage that is not directly related to the function being attacked. For example, in cryptography you might have a cryptographic function such a smart card or other software that performs communication encryption. A side channel is using information such as power analysis, timing, electromagentic emanations, sound, etc to reveal secret keys instead of attacking the cryptographic algorithm itself.\nA timing side-channel I started first with a simple kind of SCA which is a timing attack against an insecure password implementation. I found a nice blog explaining timing side channel attacks in Arduino [2] and I used the arduino code provided there, as the vulnerable implementation to attack: Arduino code from [2] The main idea is that you store a six digit password (or any length really) and you use the serial port to send a candidate password to the Arduino. While the Arduino checks the password it turns on a LED that in essence corresponds to a digital pin set to ON. The password checking implementation looks at one digit at a time but it returns as soon as an incorrect digit is found. The fact that the response is not constant in time means that it leaks information about the validity of each digit, rendering the timing attack possible. Using the fact that the LED pin is turned on while the password check is perfomed, we can monitor the time that the PIN is on using the oscilloscope. The more digits that are correct, the longer the password checker takes and the longer the LED pin is on.\nTo illustrate the problem let’s see how the Picoscope software looks like when the first 3 digits are correct: First three correct digits and when all 6 digits are correct: All digits correct Using the Picoscope The Picoscope is a USB oscilloscope that can be used for multiple purposes such as voltage measurement, spectrum analysis, serial channel decoding, etc. The interesting thing about USB scopes is that they come with powerful software that can perform advanced measurements and calculations which are usually quite expensive to get, or cost extra in bigger tabletop oscilloscopes. Of course it all comes down to specifications such as sampling rates, bandwidth, number of channels and so on, but my impression so far is that you can get very good value for money with a good USB scope.\nI do not intent to cover the Picoscope functions in detail in this blog post, but I do want to mention two functions that were important for this project: Triggers and automatic measurement functions.\nAs the scope runs, it continuously gathers data until you stop it. This results in viewing everything on screen in real time but usually we want to “freeze” the scope when something of interest happens so that we can see it. A trigger allows us to tell the scope when to start (and stop) capturing data and present them on screen. Triggers can be single (only triggers once) or repeated (when we capturing periodic events) and they come with a lot of settings such as rising edge or falling edge, etc. An example of a rising edge trigger is when we want the scope to start capturing when the voltage of a measurement goes from 0 to some higher value.\nThe other function that I used a lot in this project was the measurement functionality. Since we want to …","date":1564398965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564398965,"objectID":"a0bf076b92406ca1073feaa1016cbfe7","permalink":"https://mariarigaki.github.io/post/summer-hacakathon-2019/","publishdate":"2019-07-29T13:16:05+02:00","relpermalink":"/post/summer-hacakathon-2019/","section":"post","summary":"TL;DR Using a Picoscope 2204A and its SDK to perform timing side channel attacks against a weak password checker implementation running in an Arduino. Code, screenshots and lessons learned!\nLast weekend we held the bi-annual Stratosphere hackathon.","tags":["side-channels","oscilloscope"],"title":"Scoping it out!","type":"post"},{"authors":["Veronica Valeros","Maria Rigaki","Sebastian Garcia"],"categories":["conference paper"],"content":"","date":1561024587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553162187,"objectID":"df1f25a3892036a7eb2d941f1254e677","permalink":"https://mariarigaki.github.io/publication/machete/machete/","publishdate":"2019-06-20T11:56:27+02:00","relpermalink":"/publication/machete/machete/","section":"publication","summary":"Reports on cyber espionage operations have been on the rise in the last decade. However, operations in Latin America are heavily under researched and potentially underestimated. In this paper we analyze and dissect a cyber espionage tool known as Machete. Our research shows that Machete is operated by a highly coordinated and organized group who focuses on Latin American targets. We describe the five phases of the APT operations from delivery to exfiltration of information and we show why Machete is considered a cyber espionage tool. Furthermore, our analysis indicates that the targeted victims belong to military, political, or diplomatic sectors. The review of almost six years of Machete operations show that it is likely operated by a single group, and their activities are possibly statesponsored. Machete is still active and operational to this day","tags":["advanced persistence threat","cyber espionage","malware operations"],"title":"Machete: Dissecting the Operations of a Cyber Espionage Group in Latin America","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mariarigaki.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Maria Rigaki","Sebastian Garcia"],"categories":["conference paper"],"content":"","date":1527069387,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527069387,"objectID":"f71df3555de70f05822c3cf8e797cf9d","permalink":"https://mariarigaki.github.io/publication/gan-fight/gan-knife-fight/","publishdate":"2018-05-23T11:56:27+02:00","relpermalink":"/publication/gan-fight/gan-knife-fight/","section":"publication","summary":"Generative Adversarial Networks (GANs) have been successfully used in a large number of domains. This paper proposes the use of GANs for generating network traffic in order to mimic other types of traffic. In particular, our method modifies the network behavior of a real malware in order to mimic the traffic of a legitimate application, and therefore avoid detection. By modifying the source code of a malware to receive parameters from a GAN, it was possible to adapt the behavior of its Command and Control (C2) channel to mimic the behavior of Facebook chat network traffic. In this way, it was possible to avoid the detection of new-generation Intrusion Prevention Systems that use machine learning and behavioral characteristics. A real-life scenario was successfully implemented using the Stratosphere behavioral IPS in a router, while the malware and the GAN were deployed in the local network of our laboratory, and the C2 server was deployed in the cloud. Results show that a GAN can successfully modify the traffic of a malware to make it undetectable. The modified malware also tested if it was being blocked and used this information as a feedback to the GAN. This work envisions the possibility of self-adapting malware and self-adapting IPS.","tags":["GAN","intrusion detection"],"title":"Bringing a GAN to a Knife-fight: Adapting Malware Communication to Avoid Detection","type":"publication"},{"authors":["Maria Rigaki"],"categories":[],"content":"","date":1523059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560426163,"objectID":"36f4db920c2da7f1e3d807495e3546df","permalink":"https://mariarigaki.github.io/talk/arming-malware-with-gans/","publishdate":"2018-04-07T00:00:00Z","relpermalink":"/talk/arming-malware-with-gans/","section":"event","summary":"Generative Adversarial Networks (GANs) are a recent invention that shows impressive results in generating completely new images of faces, building interiors and much more. In this talk we present how we can use GANs to modify network traffic parameters in order to mimic other types of traffic. More specifically, we modify an open source malware to use a GAN to dynamically adapt its Command and Control network behavior and mimic the traffic characteristics of Facebook chat. In this way it is able to avoid the detection from new-generation Intrusion Prevention Systems that use behavioral characteristics. We will present our experiments from a real-life scenario that used the Stratosphere behavioral IPS deployed in a router between the malware which was deployed in our lab and the C\u0026C server deployed in AWS. Results show that it is possible for the malware to become undetected when given the input parameters from a GAN. The malware is also aware of whether or not it is being blocked and uses this as a feedback signal in order to improve the GAN model. Finally, we discuss the implications of this work in malware detection as well as other areas such as censorship circumvention.","tags":[],"title":"Arming Malware with GANs","type":"event"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://mariarigaki.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://mariarigaki.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]